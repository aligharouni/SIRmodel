We don't want to test people from the \_u compartments if they're not there.

There are two natural ways to do this. One is with a discontinuity: we define 
the weighted number of people available for tests: $W = W_s S_u + W_i I_u + W_r R_u$ and then define the scaling parameter (can we not call it ``sc'', which looks like a product?) as $sc = \textrm{H}(W) tN_0/W$, where H is the Heaviside function. This makes the system well-defined, but is still ugly in a few ways.

The second natural way is to define a time scale $\tau$ for the maximum rate of testing the whole untested population. If this is fast (e.g., 1d), it should have minimal effects on the system when we're in the usual domain. A simple way to do this is by using $sc = \frac{\tau tN_0}{\tau W + t N_0}$. Since $\tau \gg t$, this generally collapses to the original form. When $W$ is super-small, however, it collapses instead to $\tau$.

My suggested notation for this equation is $$\sigma = \frac{\tau \rho N_0}{\tau W + \rho N_0}$$

Questions/Clarifications:
(1) We have two testing rates; (i) $\tau$ and (ii) $\rho$.
$\tau$ is the testing rate when $W \approx 0$ so no one is left in the untested compartments. (Here I am not sure why "a time scale $\tau$ for the maximum rate of testing the whole untested population"?).
On the other hand $\rho$ is the testing rate at the begining of the pandemic when $W \approx N_0$. The latter requires $\tau \gg \rho$. Am I thinking right here?  

JD: Yes, you are right that $\tau \gg \rho$. But it's not exactly about the beginning of the pandemic. In general, we want to test at a rate of $\rho$ across the whole population. This won't always be possible. So we impose a maximum rate of $\tau$ per testable person. It is consistent to think of both $\tau$ and $\rho$ as pure rates, but it might be clearer to think of $\rho$ as tests per capita per unit time, and $\tau$ as tests per testable person per unit time. It's not that we're switching through time, it's that we're imposing both of these as limitations. At the beginning, we expect the answer to be close to $\rho N_0$ since $\tau$ should be very fast. Once $W$ becomes small, the limitation imposed by $\tau W$ will become important.

(2) $\tau$ has the same dimension as $\rho$ which is testing rate, thus 1/time per capita?

JD: See above. Note that if you are thinking about ``people'' in the denominator as above, you need to remember there are also people in the numerator (you can choose to be specific, or to let them cancel). But it's a bit off to have people only in the denominator.

(3) In the simmulation, I picked $\tau=1$. This made sense to me if the answeres to all above questions are "yes". Not sure what to pick if I was not thinking right in the above question.

JD: Yes, but you need units. $\tau = 1 \mathrm{day}$ is exactly what I meant by a time scale of 1 day. 
AG: Sorry, I am not sure here about the units.

(4) $\sigma$ is the testing rate with unit of 1/time. $\sigma$ has two limits
(i) When $W \approx N0$;
If we assume that $\tau \gg \rho$, $\sigma=\rho \frac{\tau N0}{(\tau+\rho)N_0} \approx \rho \frac{\tau N0}{(\tau)N_0} = \rho$. 

(ii) When $W \approx 0$;
$\sigma \approx \tau$.

(5) Right now there are no restictions on the weights, $W_S, W_I$ and $W_R$. These are representing different testing strategies. For example if $W_S=W_I=W_R$, it represents the random testing from the whole population with no tracing.
(i) Does it make sense $W_S+W_I+W_R=1$? thus if we increase the W_I, the other two weights will decrease.
(ii) I am thinking of somehow connect $W_*$ and testing rate of the whole population $\rho$. With the new definition of the $\sigma$ that JD suggested above, this is already incorporated in the rate of testing but not in the testing strategies. 

